#  End-to-End Text Summarizer Project

This is an end-to-end implementation of a **Text Summarizer** using Natural Language Processing techniques and Transformer models. The project is structured in a modular fashion and includes setup for **local execution, Dockerization, and AWS CI/CD deployment using GitHub Actions**.

---

##  Project Structure & File Explanation

Below is a detailed breakdown of the project structure and the purpose of each file/folder. 

```
Text-Summarizer-Project/
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/           # GitHub Actions workflows for CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ main.yaml               # Workflow to deploy Docker image to AWS ECR and EC2
‚îÇ
‚îú‚îÄ‚îÄ src/                         # All core Python modules for data processing and modeling
‚îÇ   ‚îú‚îÄ‚îÄ config/                  # Configuration classes for reading YAML config
‚îÇ   ‚îú‚îÄ‚îÄ components/              # Components for data ingestion, transformation, model training
‚îÇ   ‚îú‚îÄ‚îÄ constants/               # Paths and constant definitions
‚îÇ   ‚îú‚îÄ‚îÄ entity/                  # Entity definitions for structured configs
‚îÇ   ‚îú‚îÄ‚îÄ logging/                 # Custom logger setup for debugging and info tracking
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/                # Pipeline orchestrators for training and prediction
‚îÇ   ‚îî‚îÄ‚îÄ utils/                   # Utility functions used across the project
‚îÇ
‚îú‚îÄ‚îÄ app.py                       # Flask web app to interface with the model for summarization
‚îú‚îÄ‚îÄ main.py                      # Entry point to train and run the pipeline
‚îú‚îÄ‚îÄ config.yaml                  # Project configuration (paths, directories, URLs)
‚îú‚îÄ‚îÄ params.yaml                  # Model and tokenizer parameters
‚îú‚îÄ‚îÄ requirements.txt             # All Python dependencies
‚îú‚îÄ‚îÄ Dockerfile                   # Docker image setup
‚îî‚îÄ‚îÄ README.md                    # Project documentation
```

###  Explanation of Key Files and Folders

####  `config.yaml`
Contains paths for artifacts, model checkpoints, data sources, etc. It enables external configuration of project structure and helps avoid hardcoding file paths.

####  `params.yaml`
Holds parameters for tokenizer, training arguments, model selection (like BART, T5), max lengths, and batch sizes. Makes hyperparameter tuning easy and clean.

####  `src/config/`
Reads and parses values from `config.yaml` and `params.yaml` using classes. These configuration classes ensure clean data flow and modular pipeline creation.

#### `src/components/`
Core logic of the app:
- Data ingestion
- Data transformation
- Model training
- Model evaluation
- Model saving/loading

Each task is modularized into separate files.

####  `src/pipeline/`
Orchestrates the pipeline for training and prediction by calling appropriate components and passing configurations. It decouples business logic from orchestration.

####  `app.py`
This is the frontend of the project using Flask. It exposes a route (`/predict`) where the user can input long text and get the summarized output in real-time.

####  `main.py`
The main execution script that triggers the entire training pipeline, coordinating everything from data ingestion to model training.

####  `.github/workflows/main.yaml`
GitHub Actions CI/CD setup:
- Builds a Docker image on every `main` branch push
- Pushes it to AWS Elastic Container Registry (ECR)
- Pulls it onto EC2 for live deployment

---

##  Local Setup Instructions

### 1Ô∏è Clone the repository

```bash
git clone https://github.com/Bhavadharini-G/Text-Summarizer-Project
cd Text-Summarizer-Project
```

### 2Ô∏è Create a conda environment

```bash
conda create -n textS python=3.12 -y
conda activate textS
```

### 3Ô∏è Install requirements

```bash
pip install -r requirements.txt
```

### 4Ô∏è Run the Flask app

```bash
python app.py
```

Then open your browser and go to `https://locall.host/8080/`.

---



## AWS-CICD-Deployment-with-Github-Actions

### Login to AWS console.

1. Create an **IAM User** with:
   - `AmazonEC2ContainerRegistryFullAccess`
   - `AmazonEC2FullAccess`

2. Create an **ECR repository**  
   Example URI:  
   `866923973375.dkr.ecr.ap-south-1.amazonaws.com/text-s`

3. Launch an **EC2 instance (Ubuntu)**

4. Install Docker in EC2:

```bash
sudo apt-get update -y
sudo apt-get upgrade
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
newgrp docker
```

5. Set up the EC2 instance as a **self-hosted GitHub Runner** via:
```
GitHub > Settings > Actions > Runners > self-hosted 
```

### GitHub Secrets Configuration

- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`
- `AWS_ECR_LOGIN_URI`
- `ECR_REPOSITORY_NAME`

---

##  GitHub Actions Workflow (`.github/workflows/main.yaml`)

This file automates:
- Docker image build
- Login to ECR
- Push image to ECR

It allows seamless deployment whenever new code is pushed to `main`.

---

## üñºÔ∏è Sample Output ‚Äì Swagger UI

Below is the sample output from the FastAPI Swagger interface:

![Sample Output](assets/Sample_output.png)




##  Author

**Bhavadharini G**  
Data Science Student, M.Tech  
gunasekaranbhavadharini@gmail.com  

---

##  License

This project is released under the [MIT License](LICENSE).

---
